spring:
  application:
    name: consumer
  cloud:
    stream:
      function:
        definition: process
      bindings:
        process-in-0:
          destination: sensor-topic
          group: ${spring.application.name}
          content-type: application/*+avro
#          Commenting out this allows schema evolution to work properly but the schema and messages
#          produced in Kafka are just bytes, so you can't view the schema with Confluent Control Center
          consumer:
            useNativeEncoding: true

      kafka:
        binder:
          autoAddPartitions: true
          minPartitionCount: 8
        bindings:
          process-in-0:
            consumer:
              configuration:
                specific.avro.reader: true
                key.serializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
                value.deserializer: org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
                spring.deserializer.value.delegate.class: io.confluent.kafka.serializers.KafkaAvroDeserializer
                schema.registry.url: http://localhost:8081
                auto.register.schemas: true

server:
  port: 0